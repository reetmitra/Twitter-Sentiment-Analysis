{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/reetmitra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/reetmitra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/reetmitra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target         ids                          date      flag  \\\n",
      "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "\n",
      "              user                                               text  \n",
      "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
      "Cleaned and tokenized data saved to ../data/processed/cleaned_tokenized_sentiment140.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 100000 entries, 541200 to 429504\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   target        100000 non-null  int64 \n",
      " 1   ids           100000 non-null  int64 \n",
      " 2   date          100000 non-null  object\n",
      " 3   flag          100000 non-null  object\n",
      " 4   user          100000 non-null  object\n",
      " 5   text          100000 non-null  object\n",
      " 6   cleaned_text  100000 non-null  object\n",
      " 7   tokens        100000 non-null  object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 6.9+ MB\n",
      "None\n",
      "        target         ids                          date      flag  \\\n",
      "541200       0  2200003196  Tue Jun 16 18:18:12 PDT 2009  NO_QUERY   \n",
      "750          0  1467998485  Mon Apr 06 23:11:14 PDT 2009  NO_QUERY   \n",
      "766711       0  2300048954  Tue Jun 23 13:40:11 PDT 2009  NO_QUERY   \n",
      "285055       0  1993474027  Mon Jun 01 10:26:07 PDT 2009  NO_QUERY   \n",
      "705995       0  2256550904  Sat Jun 20 12:56:51 PDT 2009  NO_QUERY   \n",
      "\n",
      "                   user                                               text  \\\n",
      "541200  LaLaLindsey0609             @chrishasboobs AHHH I HOPE YOUR OK!!!    \n",
      "750         sexygrneyes  @misstoriblack cool , i have no tweet apps  fo...   \n",
      "766711       sammydearr  @TiannaChaos i know  just family drama. its la...   \n",
      "285055      Lamb_Leanne  School email won't open  and I have geography ...   \n",
      "705995      yogicerdito                             upper airways problem    \n",
      "\n",
      "                                             cleaned_text  \\\n",
      "541200                               ahhh i hope your ok    \n",
      "750              cool  i have no tweet apps  for my razr    \n",
      "766711   i know  just family drama its lamehey next ti...   \n",
      "285055  school email wont open  and i have geography s...   \n",
      "705995                             upper airways problem    \n",
      "\n",
      "                                                   tokens  \n",
      "541200                                       ahhh hope ok  \n",
      "750                                  cool tweet apps razr  \n",
      "766711  know family drama lamehey next time u hang kim...  \n",
      "285055  school email wont open geography stuff revise ...  \n",
      "705995                               upper airway problem  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK data files (only need to run once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/processed/processed_sentiment140.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the text column\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "df['tokens'] = df['cleaned_text'].apply(word_tokenize)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "\n",
    "# Lemmatize the tokens\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['tokens'] = df['tokens'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "# Join tokens into a single string\n",
    "df['tokens'] = df['tokens'].apply(' '.join)\n",
    "\n",
    "# Limit the dataset size (adjust as needed)\n",
    "df = df.sample(n=100000, random_state=42)\n",
    "\n",
    "# Save the cleaned and tokenized data\n",
    "df.to_csv('../data/processed/cleaned_tokenized_sentiment140.csv', index=False)\n",
    "print(\"Cleaned and tokenized data saved to ../data/processed/cleaned_tokenized_sentiment140.csv\")\n",
    "\n",
    "# Display basic information about the processed dataset\n",
    "print(df.info())\n",
    "\n",
    "# Display the first few rows of the processed dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
